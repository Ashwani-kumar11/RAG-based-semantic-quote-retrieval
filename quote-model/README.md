---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- dense
- generated_from_trainer
- dataset_size:2499
- loss:MultipleNegativesRankingLoss
widget:
- source_sentence: quotes about humor paranormal-romance by cassandra clare,
  sentences:
  - ‚Äúyou know, hobbes, some days even my lucky rocket ship underpants don't help.‚Äù
  - ‚Äúyou guessed? you must have been pretty sure, considering you could have killed
    me.""i was ninety percent sure.""i see," clary said. there must have been something
    in her voice, because he turned to look at her. her hand cracked across his face,
    a slap that rocked him back on his heels. he put his hands on his cheek, more
    in surprise than pain."what the hell was that for?""the other ten percent.‚Äù
  - ‚Äúit was love at first sight, at last sight, at ever and ever sight.‚Äù
- source_sentence: quotes about city-of-bones simon-lewis by cassandra clare,
  sentences:
  - ‚Äúbelieve in your infinite potential. your only limitations are those you set upon
    yourself.‚Äù
  - ‚Äúthat's why when major badasses greet each other in movies, they don't say anything,
    they just nod. the nod means, 'i' am a badass, and i recognize that you, too,
    are a badass,' but they don't say anything because they're wolverine and magneto
    and it would mess up their vibe to explain.‚Äù
  - ‚Äúto live is the rarest thing in the world. most people exist, that is all.‚Äù
- source_sentence: quotes about beauty david by scott westerfeld,
  sentences:
  - ‚Äúkeep your face always toward the sunshine - and shadows will fall behind you.‚Äù
  - ‚Äúa house without books is like a room without windows.‚Äù
  - ‚Äúwhat you do, the way you think, makes you beautiful.‚Äù
- source_sentence: quotes about individuality by joss whedon
  sentences:
  - ‚Äúremember to always be yourself. unless you suck.‚Äù
  - ‚Äúfriendship is unnecessary, like philosophy, like art.... it has no survival value;
    rather it is one of those things which give value to survival.‚Äù
  - ‚Äúlisten with curiosity. speak with honesty. act with integrity. the greatest problem
    with communication is we don√¢‚Ç¨‚Ñ¢t listen to understand. we listen to reply. when
    we listen with curiosity, we don√¢‚Ç¨‚Ñ¢t listen with the intent to reply. we listen
    for what√¢‚Ç¨‚Ñ¢s behind the words.‚Äù
- source_sentence: quotes about empathy friends by henri nouwen,
  sentences:
  - ‚Äúwhen we honestly ask ourselves which person in our lives mean the most to us,
    we often find that it is those who, instead of giving advice, solutions, or cures,
    have chosen rather to share our pain and touch our wounds with a warm and tender
    hand. the friend who can be silent with us in a moment of despair or confusion,
    who can stay with us in an hour of grief and bereavement, who can tolerate not
    knowing, not curing, not healing and face with us the reality of our powerlessness,
    that is a friend who cares.‚Äù
  - ‚Äúi know some who are constantly drunk on books as other men are drunk on whiskey.‚Äù
  - ‚Äúat some point, you just pull off the band-aid, and it hurts, but then it's over
    and you're relieved.‚Äù
pipeline_tag: sentence-similarity
library_name: sentence-transformers
---

# SentenceTransformer

This is a [sentence-transformers](https://www.SBERT.net) model trained. It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
<!-- - **Base model:** [Unknown](https://huggingface.co/unknown) -->
- **Maximum Sequence Length:** 256 tokens
- **Output Dimensionality:** 384 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ü§ó Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'quotes about empathy friends by henri nouwen,',
    '‚Äúwhen we honestly ask ourselves which person in our lives mean the most to us, we often find that it is those who, instead of giving advice, solutions, or cures, have chosen rather to share our pain and touch our wounds with a warm and tender hand. the friend who can be silent with us in a moment of despair or confusion, who can stay with us in an hour of grief and bereavement, who can tolerate not knowing, not curing, not healing and face with us the reality of our powerlessness, that is a friend who cares.‚Äù',
    '‚Äúi know some who are constantly drunk on books as other men are drunk on whiskey.‚Äù',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[ 1.0000,  0.5362,  0.0242],
#         [ 0.5362,  1.0000, -0.0151],
#         [ 0.0242, -0.0151,  1.0000]])
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 2,499 training samples
* Columns: <code>sentence_0</code> and <code>sentence_1</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                        | sentence_1                                                                         |
  |:--------|:----------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|
  | type    | string                                                                            | string                                                                             |
  | details | <ul><li>min: 7 tokens</li><li>mean: 11.68 tokens</li><li>max: 31 tokens</li></ul> | <ul><li>min: 9 tokens</li><li>mean: 39.69 tokens</li><li>max: 256 tokens</li></ul> |
* Samples:
  | sentence_0                                                                    | sentence_1                                                                                                                                                                                                                                                                               |
  |:------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | <code>quotes about jem-carstairs will-herondale by cassandra clare,</code>    | <code>‚Äúremember when you tried to convince me to feed a poultry pie to the mallards in the park to see if you could breed a race of cannibal ducks?" "they ate it too," will reminisced. "bloodthirsty little beasts. never trust a duck.‚Äù</code>                                        |
  | <code>quotes about the-perks-of-being-a-wallflower by stephen chbosky,</code> | <code>‚Äúso, i guess we are who we are for alot of reasons. and maybe we'll never know most of them. but even if we don't have the power to choose where we come from, we can still choose where we go from there. we can still do things. and we can try to feel okay about them.‚Äù</code> |
  | <code>quotes about advice-for-writers awareness by leo tolstoy,</code>        | <code>‚Äúif, then, i were asked for the most important advice i could give, that which i considered to be the most useful to the men of our century, i should simply say: in the name of god, stop a moment, cease your work, look around you.‚Äù</code>                                     |
* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:
  ```json
  {
      "scale": 20.0,
      "similarity_fct": "cos_sim"
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `num_train_epochs`: 10
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 10
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `hub_revision`: None
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `liger_kernel_config`: None
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: False
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin
- `router_mapping`: {}
- `learning_rate_mapping`: {}

</details>

### Training Logs
| Epoch  | Step | Training Loss |
|:------:|:----:|:-------------:|
| 3.1847 | 500  | 0.7185        |
| 6.3694 | 1000 | 0.374         |
| 9.5541 | 1500 | 0.2544        |


### Framework Versions
- Python: 3.12.8
- Sentence Transformers: 5.0.0
- Transformers: 4.53.1
- PyTorch: 2.5.1+cpu
- Accelerate: 1.8.1
- Datasets: 3.6.0
- Tokenizers: 0.21.2

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### MultipleNegativesRankingLoss
```bibtex
@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply},
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->